{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "A minimal, self‑contained toy implementation of **Artificial Kuramoto Oscillatory Neurons (AKOrN)**\n",
    "showing how to embed a single AKOrN layer inside a small network and train it on the classic\n",
    "`two‑moons` toy dataset.  The code runs on CPU or GPU (if available) and should finish in a\n",
    "few seconds.\n",
    "\n",
    "Key simplifications w.r.t. the full paper:\n",
    "1. Each oscillatory neuron lives on a *2‑sphere* (d = 2).  This lets us represent the\n",
    "   phase directly by a 2‑D unit vector `[cos θ, sin θ]`.\n",
    "2. Coupling weights are learned but *shared across the batch* and soft‑symmetrized via `tanh`.\n",
    "3. A single AKOrN layer is followed by a linear read‑out that flattens the phase vectors.\n",
    "4. We use a fixed number of Kuramoto steps (`steps=5`) and an explicit Euler integrator.\n",
    "\n",
    "Despite these simplifications the code reproduces the AKOrN spirit:\n",
    "• Neurons rotate and synchronize through Kuramoto coupling.  \n",
    "• The Lyapunov‑like norm constraint is enforced by `F.normalize` after each time step.  \n",
    "• Test‑time we can simply increase `steps` to perform extra inference iterations.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                             Core AKOrN layer\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class AKOrNLayer(nn.Module):\n",
    "    \"\"\"Kuramoto‑style oscillatory neurons on the unit d‑sphere.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        d: int = 2,\n",
    "        steps: int = 5,\n",
    "        dt: float = 1.0,\n",
    "        coupling_scale: float = 1.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.d = d\n",
    "        self.steps = steps\n",
    "        self.dt = dt\n",
    "\n",
    "        # Linear projection from input to initial latent phases (before normalization)\n",
    "        self.W_in = nn.Parameter(torch.randn(in_features, out_features, d) * 0.2)\n",
    "        # Learned intrinsic frequency for each neuron\n",
    "        self.omega = nn.Parameter(torch.zeros(out_features, d))\n",
    "        # Coupling matrix (learned, will be symmetrized via tanh)\n",
    "        self.coupling = nn.Parameter(torch.randn(out_features, out_features) * coupling_scale)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x shape (batch, in_features)\n",
    "        # (1) Project to d‑dimensional latent and normalize to unit length\n",
    "        # Shape: (batch, out_features, d)\n",
    "        v = torch.einsum(\"bi,iod->bod\", x, self.W_in)\n",
    "        v = F.normalize(v, dim=-1)\n",
    "\n",
    "        # (2) Kuramoto iterations (explicit Euler)\n",
    "        # Pre‑compute symmetric coupling (same for every batch element)\n",
    "        K = torch.tanh(self.coupling)  # bounds the magnitude and softly symmetrizes\n",
    "        for _ in range(self.steps):\n",
    "            # Aggregate neighbors: sum_j K_ij * v_j\n",
    "            agg = torch.einsum(\"ij,bjd->bid\", K, v)\n",
    "            # Euler update and re‑normalize (unit‑sphere constraint)\n",
    "            v = v + self.dt * (agg + self.omega)  # intrinsic + coupled rotation\n",
    "            v = F.normalize(v, dim=-1)\n",
    "        return v  # shape (batch, out_features, d)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                             Tiny AKOrN network\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ToyAKOrNNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden: int, num_classes: int, steps: int = 5):\n",
    "        super().__init__()\n",
    "        self.akorn = AKOrNLayer(input_dim, hidden, d=2, steps=steps)\n",
    "        # Flatten the (hidden,2) oscillators and map to class logits\n",
    "        self.readout = nn.Linear(hidden * 2, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        v = self.akorn(x)            # (batch, hidden, 2)\n",
    "        v_flat = v.flatten(1)        # (batch, hidden*2)\n",
    "        return self.readout(v_flat)  # logits\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                             Training utilities\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    hidden: int = 32\n",
    "    steps: int = 5         # Kuramoto steps during *training*\n",
    "    epochs: int = 200\n",
    "    lr: float = 1e-2\n",
    "    test_steps: int = 20   # extra inference iterations at test‑time\n",
    "\n",
    "\n",
    "def make_dataloaders(test_size: float = 0.3, batch_size: int = 128) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    X, y = make_moons(n_samples=1500, noise=0.2, random_state=0)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
    "    test_ds = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test))\n",
    "\n",
    "    return (\n",
    "        torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_toy_model(cfg: TrainConfig) -> None:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    train_loader, test_loader = make_dataloaders()\n",
    "    model = ToyAKOrNNet(input_dim=2, hidden=cfg.hidden, num_classes=2, steps=cfg.steps).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            acc = evaluate(model, test_loader, device)\n",
    "            print(f\"Epoch {epoch+1:4d} | train loss {train_loss:.4f} | test acc {acc:.2%}\")\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    # Extra inference iterations (test‑time refinement) -----------------------\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    model.akorn.steps = cfg.test_steps  # increase Kuramoto iterations\n",
    "    refined_acc = evaluate(model, test_loader, device)\n",
    "    print(\"Final accuracy after extra inference iterations:\", f\"{refined_acc:.2%}\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: torch.utils.data.DataLoader, device: str) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = TrainConfig()\n",
    "    train_toy_model(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
